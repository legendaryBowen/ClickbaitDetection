{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "q-YbjCkzw0yU"
   },
   "outputs": [],
   "source": [
    "# A dependency of the preprocessing for BERT inputs\n",
    "!pip install -q tensorflow-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "b-P1ZOA0FkVJ"
   },
   "outputs": [],
   "source": [
    "!pip install -q tf-models-official"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "_XgTpm9ZxoN9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yuanb\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:68: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.2.0 and strictly below 2.4.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.4.0-rc4 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  UserWarning,\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from official.nlp import optimization  # to create AdamW optmizer\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import json\n",
    "from utils import *\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "xtXcJ1xMy66V"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'train.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-19644047d063>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtrain_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'postText'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'truthMedian'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'truthClass'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'truthMean'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'train.json'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjson_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0minstance\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mtrain_instance\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0minstance\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text_id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'postText'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0minstance\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'post_text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'truthMedian'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0minstance\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'truth_median'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'truthClass'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0minstance\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'click_bait'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'truthMean'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0minstance\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'truth_mean'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'train.json'"
     ]
    }
   ],
   "source": [
    "train_df = pd.DataFrame(columns=['id','postText','truthMedian','truthClass','truthMean'])\n",
    "with open('data/train.json') as json_data:\n",
    "    data = json.load(json_data)\n",
    "    for instance in data:\n",
    "        train_instance = {'id': instance['text_id'], 'postText': instance['post_text'], 'truthMedian': instance['truth_median'], 'truthClass': instance['click_bait'], 'truthMean': instance['truth_mean']}\n",
    "        train_df = train_df.append(train_instance, ignore_index = True)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jvkjmYQEzE-a"
   },
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame(columns=['id','postText','truthMedian','truthClass','truthMean'])\n",
    "with open('data/test.json') as json_data:\n",
    "    data = json.load(json_data)\n",
    "    for instance in data:\n",
    "        test_instance = {'id': instance['text_id'], 'postText': instance['post_text'], 'truthMedian': instance['truth_median'], 'truthClass': instance['click_bait'], 'truthMean': instance['truth_mean']}\n",
    "        test_df = test_df.append(test_instance, ignore_index = True)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fUwE6ExFzGU-"
   },
   "outputs": [],
   "source": [
    "# Remove all punctuations and clear the text\n",
    "train_df['postText'] = train_df['postText'].apply(cleanText)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H2H3fdTYzKa2"
   },
   "outputs": [],
   "source": [
    "# Remove all punctuations and clear the text\n",
    "test_df['postText'] = test_df['postText'].apply(cleanText)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "whsVTXQqzNd9"
   },
   "outputs": [],
   "source": [
    "X_train, Y_train = np.array(train_df[\"postText\"].tolist()), np.array(train_df[\"truthClass\"].tolist())\n",
    "X_test, Y_test = np.array(test_df[\"postText\"].tolist()), np.array(test_df[\"truthClass\"].tolist())\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v3ojA3HBq53A"
   },
   "outputs": [],
   "source": [
    "print(X_test[0:10])\n",
    "print(Y_test[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y8_ctG55-uTX"
   },
   "outputs": [],
   "source": [
    "#@title Choose a BERT model to fine-tune\n",
    "\n",
    "bert_model_name = 'small_bert/bert_en_uncased_L-4_H-512_A-8'  #@param [\"bert_en_uncased_L-12_H-768_A-12\", \"bert_en_cased_L-12_H-768_A-12\", \"bert_multi_cased_L-12_H-768_A-12\", \"small_bert/bert_en_uncased_L-2_H-128_A-2\", \"small_bert/bert_en_uncased_L-2_H-256_A-4\", \"small_bert/bert_en_uncased_L-2_H-512_A-8\", \"small_bert/bert_en_uncased_L-2_H-768_A-12\", \"small_bert/bert_en_uncased_L-4_H-128_A-2\", \"small_bert/bert_en_uncased_L-4_H-256_A-4\", \"small_bert/bert_en_uncased_L-4_H-512_A-8\", \"small_bert/bert_en_uncased_L-4_H-768_A-12\", \"small_bert/bert_en_uncased_L-6_H-128_A-2\", \"small_bert/bert_en_uncased_L-6_H-256_A-4\", \"small_bert/bert_en_uncased_L-6_H-512_A-8\", \"small_bert/bert_en_uncased_L-6_H-768_A-12\", \"small_bert/bert_en_uncased_L-8_H-128_A-2\", \"small_bert/bert_en_uncased_L-8_H-256_A-4\", \"small_bert/bert_en_uncased_L-8_H-512_A-8\", \"small_bert/bert_en_uncased_L-8_H-768_A-12\", \"small_bert/bert_en_uncased_L-10_H-128_A-2\", \"small_bert/bert_en_uncased_L-10_H-256_A-4\", \"small_bert/bert_en_uncased_L-10_H-512_A-8\", \"small_bert/bert_en_uncased_L-10_H-768_A-12\", \"small_bert/bert_en_uncased_L-12_H-128_A-2\", \"small_bert/bert_en_uncased_L-12_H-256_A-4\", \"small_bert/bert_en_uncased_L-12_H-512_A-8\", \"small_bert/bert_en_uncased_L-12_H-768_A-12\", \"albert_en_base\", \"electra_small\", \"electra_base\", \"experts_pubmed\", \"experts_wiki_books\", \"talking-heads_base\"]\n",
    "\n",
    "map_name_to_handle = {\n",
    "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1'\n",
    "}\n",
    "\n",
    "map_model_to_preprocess = {\n",
    "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1'\n",
    "}\n",
    "\n",
    "tfhub_handle_encoder = map_name_to_handle[bert_model_name]\n",
    "tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n",
    "\n",
    "print(f'BERT model selected           : {tfhub_handle_encoder}')\n",
    "print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0SQi-jWd_jzq"
   },
   "outputs": [],
   "source": [
    "bert_preprocess_model = hub.KerasLayer(tfhub_handle_preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r9-zCzJpnuwS"
   },
   "outputs": [],
   "source": [
    "# text_test = ['this is such an amazing movie!']\n",
    "# text_preprocessed = bert_preprocess_model(text_test)\n",
    "\n",
    "# print(f'Keys       : {list(text_preprocessed.keys())}')\n",
    "# print(f'Shape      : {text_preprocessed[\"input_word_ids\"].shape}')\n",
    "# print(f'Word Ids   : {text_preprocessed[\"input_word_ids\"][0, :12]}')\n",
    "# print(f'Input Mask : {text_preprocessed[\"input_mask\"][0, :12]}')\n",
    "# print(f'Type Ids   : {text_preprocessed[\"input_type_ids\"][0, :12]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tXxYpK8ixL34"
   },
   "outputs": [],
   "source": [
    "bert_model = hub.KerasLayer(tfhub_handle_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_OoF9mebuSZc"
   },
   "outputs": [],
   "source": [
    "# bert_results = bert_model(text_preprocessed)\n",
    "\n",
    "# print(f'Loaded BERT: {tfhub_handle_encoder}')\n",
    "# print(f'Pooled Outputs Shape:{bert_results[\"pooled_output\"].shape}')\n",
    "# print(f'Pooled Outputs Values:{bert_results[\"pooled_output\"][0, :12]}')\n",
    "# print(f'Sequence Outputs Shape:{bert_results[\"sequence_output\"].shape}')\n",
    "# print(f'Sequence Outputs Values:{bert_results[\"sequence_output\"][0, :12]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aksj743St9ga"
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout\n",
    "def build_classifier_model():\n",
    "  text_input = Input(shape=(), dtype=tf.string, name='text')\n",
    "  preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
    "  encoder_inputs = preprocessing_layer(text_input)\n",
    "  encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n",
    "  outputs = encoder(encoder_inputs)\n",
    "  net = outputs['pooled_output']\n",
    "  net = Dropout(0.1)(net)\n",
    "  net = Dense(1, activation=None, name='classifier')(net)\n",
    "  return Model(text_input, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mGMF8AZcB2Zy"
   },
   "outputs": [],
   "source": [
    "bert_classifier = build_classifier_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0EmzyHZXKIpm"
   },
   "outputs": [],
   "source": [
    "bert_classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P9eP2y9dbw32"
   },
   "outputs": [],
   "source": [
    "steps_per_epoch = X_train.shape[0]\n",
    "epochs = 5\n",
    "num_train_steps = steps_per_epoch * epochs\n",
    "num_warmup_steps = int(0.1*num_train_steps)\n",
    "init_lr = 3e-4\n",
    "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
    "                                          num_train_steps=num_train_steps,\n",
    "                                          num_warmup_steps=num_warmup_steps,\n",
    "                                          optimizer_type='adamw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-0MTTK3cyQN5"
   },
   "outputs": [],
   "source": [
    "loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "metrics = tf.metrics.BinaryAccuracy()\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-7GPDhR98jsD"
   },
   "outputs": [],
   "source": [
    "bert_classifier.compile(optimizer=optimizer, loss=loss, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HtfDFAnN_Neu"
   },
   "outputs": [],
   "source": [
    "print(f'Training model with {tfhub_handle_encoder}')\n",
    "history = bert_classifier.fit(X_train, Y_train, validation_split=0.2, epochs=10, batch_size=64, callbacks=[callback], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "slqB-urBV9sP"
   },
   "outputs": [],
   "source": [
    "loss, accuracy = bert_classifier.evaluate(X_test, Y_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
    "loss, accuracy = bert_classifier.evaluate(X_test, Y_test)\n",
    "print(f'Loss: {loss}')\n",
    "print(f'Accuracy: {accuracy}')\n",
    "Y_predict = bert_classifier.predict(X_test)\n",
    "Y_predict_binary = [1 if i >= 0.5 else 0 for [i] in Y_predict]\n",
    "print(\"Bert Testing report\")\n",
    "print(classification_report(Y_test, Y_predict_binary))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fiythcODf0xo"
   },
   "outputs": [],
   "source": [
    "history_dict = history.history\n",
    "print(history_dict.keys())\n",
    "\n",
    "acc = history_dict['binary_accuracy']\n",
    "val_acc = history_dict['val_binary_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "fig = plt.figure(figsize=(10, 6))\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.subplot(2, 1, 1)\n",
    "# \"bo\" is for \"blue dot\"\n",
    "plt.plot(epochs, loss, 'r', label='Training loss')\n",
    "# b is for \"solid blue line\"\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "# plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(epochs, acc, 'r', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gUEWVskZjEF0"
   },
   "outputs": [],
   "source": [
    "bert_classifier.save(\"BERT_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ERNPtjjkvLQL"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Bert_Clickbait_Detector.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
